{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence Labeling: Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding: \n",
    "    - relation between Sequence Labeling and Shallow Parsing\n",
    "    - IOB Notation\n",
    "    - Joint Segmentation and Classification\n",
    "    - Feature Engineering\n",
    "- Learning how to:\n",
    "    - use Named Entity Recognition in \n",
    "        - spacy\n",
    "        - NLTK\n",
    "    - train, test, and evaluate Conditional Random Fields models\n",
    "    - perform feature engineering with CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)\n",
    "- Conditional Random Fields\n",
    "    - Lafferty et al. (2001) [Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.803&rep=rep1&type=pdf) (__original paper__)\n",
    "    - Sutton & McCallum's [An Introduction to Conditional Random Fields](https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf)\n",
    "    - Edwin Chen's [Introduction to Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)\n",
    "    - Michael Collin's [Log-Linear Models, MEMMs, and CRFs](http://www.cs.columbia.edu/~mcollins/crf.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 8: Sequence Labeling for Parts of Speech and Named Entities](https://web.stanford.edu/~jurafsky/slp3/8.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 7: Extracting Information from Text](https://www.nltk.org/book/ch07.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [`conll.py`](https://github.com/esrel/LUS/) (in `src` folder)\n",
    "- [CRFsuite](http://www.chokkan.org/software/crfsuite/)\n",
    "    - [python-crfsuite](https://python-crfsuite.readthedocs.io) python binding to `CRFsuite`.\n",
    "    - [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io) `python-crfsuite` wrapper providing API similar to `scikit-learn`\n",
    "    - you need to install both `python_crfsuite` and `sklearn_crfsuite` to use `sklearn_crfsuite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Sequence Labeling and Shallow Parsing\n",
    "\n",
    "Below are some examples of NLP tasks that Sequence Labeling is applied to as one of the methods.\n",
    "\n",
    "The scenario when members of a sequence are mapped to higher order units (i.e. grouped together `[['a'],['b','c']]`) and assigned a category) is known as __shallow parsing__.\n",
    "\n",
    "- [Part-of-Speech Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "- [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) (Chunking)\n",
    "    - [Phrase Chunking](https://en.wikipedia.org/wiki/Phrase_chunking)\n",
    "    - [Named-Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) \n",
    "    - [Semantic Role Labeling](https://en.wikipedia.org/wiki/Semantic_role_labeling)\n",
    "    - Dependency [Parsing](https://en.wikipedia.org/wiki/Parsing) \n",
    "    - Discourse Parsing\n",
    "    - (Natural/Spoken) __Language Understanding__: Concept Tagging/Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. The General Setting for Sequence Labeling\n",
    "\n",
    "- Create __training__ and __testing__ sets by tagging a certain amount of text by hand\n",
    "    - i.e. map each word in corpus to a tag\n",
    "- Train tagging model to extract generalizations from the annotated __training__ set\n",
    "- Evaluate the trained tagging model on the annotated __testing__ set\n",
    "- Use the trained tagging model too annotate new texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Shallow Parsing\n",
    "\n",
    "As we have already mentioned, [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) is a kind of Sequence Labeling. The main difference from Sequence Labeling task, such as Part-of-Speech tagging, where there is an output label (tag) per token; Shallow Parsing additionally performs __chunking__ -- segmentation of input sequence into constituents. Chunking is required to identify categories (or types) of *multi-word expressions*.\n",
    "In other words, we want to be able to capture information that expressions like `\"New York\"` that consist of 2 tokens, constitute a single unit.\n",
    "\n",
    "What this means in practice is that Shallow Parsing performs *jointly* (or not) 2 tasks:\n",
    "- __Segmentation__ of input into constituents (__spans__)\n",
    "- __Classification__ (Categorization, Labeling) of these constituents into predefined set of labels (__types__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Encoding Segmentation Information: CoNLL Corpus Format\n",
    "\n",
    "Corpus in CoNLL format consists of series of sentences, separated by blank lines. Each sentence is encoded using a table (or \"grid\") of values, where each line corresponds to a single word, and each column corresponds to an annotation type. \n",
    "\n",
    "The set of columns used by CoNLL-style files can vary from corpus to corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    I-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. [IOB Scheme](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))\n",
    "\n",
    "- The notation scheme is used to label *multi-word* spans in token-per-line format.\n",
    "    - *Los Angeles* is a *LOCATION* concept that has 2 tokens\n",
    "- Both, prefix and suffix notations are commons: \n",
    "    - prefix: __B-LOC__\n",
    "    - suffix: __LOC-B__\n",
    "\n",
    "- Meaning of Prefixes\n",
    "    - __B__ for (__B__)eginning of span\n",
    "    - __I__ for (__I__)nside of span\n",
    "    - __O__ for (__O__)utside of span (no prefix or suffix, just `O`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Alternative Schemes:\n",
    "- No prefix or suffix (useful when there are no *multi-word* concepts)\n",
    "```\n",
    "        Alex       PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        LOC\n",
    "        Angeles    LOC\n",
    "        in         O\n",
    "        California LOC\n",
    "```\n",
    "- __IOB/IOB2/BIO__\n",
    "\n",
    "- __IOBE__\n",
    "    - IOB + \n",
    "    - __E__ for (__E__)nd of span (or __L__ for (__L__)ast)\n",
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    E-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```\n",
    "    \n",
    "- __BILOU/BIOES__\n",
    "    - IOB + \n",
    "    - __L__ for (__L__)ast word of span\n",
    "    - __U__ for (__U__)nit word (or __S__ for (__S__)ingleton)\n",
    "```\n",
    "        Alex       U-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    E-LOC\n",
    "        in         O\n",
    "        California U-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Choice of Scheme\n",
    "- It is possible to convert IOB, IOBE, & BILOU formats to each other\n",
    "- Each prefix is applied to every concept label, consequently we increase the number of transitions whose probabilities we need to estimate; \n",
    "    - increasing data sparseness, as for each label we will have less observations\n",
    "- The choice of scheme depends on the amount of available data:\n",
    "    - __IOB__ for least amount\n",
    "    - __BILOU__ for the most amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Terminology\n",
    "There is no strict naming convention regarding schemes (see alternatives) or how each constituent is termed. \n",
    "Below is the terminology used in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "        Alex       B-PER\n",
    "        is         O\n",
    "        going      O\n",
    "        to         O\n",
    "        Los        B-LOC\n",
    "        Angeles    I-LOC\n",
    "        in         O\n",
    "        California B-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Interpretation\n",
    "Segmentation and Labeling data formats encode the following information:\n",
    "- in string (sentence) `\"Alex is going to Los Angeles in California\"`\n",
    "- there are 3 __entities__ (a.k.a. chunks, concepts or slots, depending on NLP task and perspective), that have __types__ (labels)\n",
    "    - `PERSON`\n",
    "    - `LOCATION`\n",
    "    \n",
    "- entity of __type__ `PERSON`: \n",
    "    - has __span__:\n",
    "        - as tokens from `0` for *CoNLL*: `[0:1]`\n",
    "    - has __value__: `\"Alex\"`\n",
    "        - string *covered by* (*on included*) in __span__\n",
    " \n",
    "*CoNLL* format encodes __tokenization__ informations. In other words, how string `\"Alex is going to Los Angeles in California\"` is split into tokens. Since most Sequence Labeling algorithms operate on token level, internally the strings are split into tokens, applying *IOB*-like schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Wirte the IOB tags for the sentence `\"Steve Jobs established Apple Inc.\"`\n",
    "    - 'Steve Jobs' is PER\n",
    "    - 'Apple Inc' is ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Named Entity Recognition with NLTK\n",
    "[NLTK](https://www.nltk.org/api/nltk.tag.html) provides implementations of popular sequence labeling algorithms for Part-of-Speech Tagging (including [HMM](https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.hmm)), that can be used for Sequence Labeling in general. \n",
    "\n",
    "- Loading & working with CoNLL format corpora in NLTK\n",
    "- Tagger training & testing (running)\n",
    "\n",
    "To have a custom tagger that labels input text with our __custom label set__, we need to __train__ it on a corpus annotated with this __custom label set__.\n",
    "\n",
    "Addtionally, NLTK provides [Chunking](http://www.nltk.org/api/nltk.chunk.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. NLTK Pre-trained NE Chunker\n",
    "\n",
    "NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function `nltk.ne_chunk()`. If we set the parameter `binary=True`, then named entities are just tagged as `NE`; otherwise, the classifier adds category labels such as `PERSON`, `ORGANIZATION`, and `GPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "for s in treebank.tagged_sents():\n",
    "    print(s)\n",
    "    print(nltk.ne_chunk(s))\n",
    "    print(nltk.ne_chunk(s, binary=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Training NLTK Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "\n",
    "print(len(conll2002.tagged_sents()))\n",
    "print(conll2002._chunk_types)\n",
    "print(conll2002.sents('esp.train')[0])\n",
    "print(conll2002.tagged_sents('esp.train')[0])\n",
    "print(conll2002.chunked_sents('esp.train')[0])\n",
    "print(conll2002.iob_sents('esp.train')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# training hmm on training data: exactly as above\n",
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "print(conll2002.iob_sents('esp.train')[0])\n",
    "\n",
    "# let's get only word and iob-tag\n",
    "trn_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "print(trn_sents[0])\n",
    "\n",
    "tst_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "\n",
    "hmm_ner = hmm_model.train(trn_sents)\n",
    "    \n",
    "# evaluation\n",
    "accuracy = hmm_ner.accuracy(tst_sents)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### NLTK Chunk Tagger Note\n",
    "HMM uses only words as input, NLTK also povides trainable MaxEnt Chunker Tagger, which unfortunatelly requires `megam` file. Unfortunatelly, it is very convoluted to install. (http://www.umiacs.umd.edu/~hal/megam/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Segmentation \n",
    "Train a tagger to perform *segmentation* of input sentences into constituents\n",
    "- Strip concept information from output labels (i.e. keep only IOB-prefix)\n",
    "- Train tagger to predict segmentation labels\n",
    "- Evaluate segmentation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "print(conll2002.iob_sents('esp.train')[0])\n",
    "\n",
    "# let's get only word and iob-tag\n",
    "# Hint: split iob with using '-'\n",
    "trn_sents_seg = [[(text, #Replace with the requested label) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "print(trn_sents[0])\n",
    "\n",
    "tst_sents_seg = [[(text, #Replace with the requested label) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "\n",
    "hmm_seg = # Train the model using train segments\n",
    "    \n",
    "# evaluation\n",
    "accuracy = # Compute accuracy using test segments\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CoNLL Eval\n",
    "CoNLL Community developed a perl script to evaluate *segmentation* and *labeling* performance jointly using IOB information. Such evaluation provides more accurate assessment of the shallow parsing performance, in comparison to token-level metrics (e.g. NLTK accuracy).\n",
    "\n",
    "- import `evaluate` function from `conll.py` (example shown)\n",
    "- evaluate tagger predictions\n",
    "- compare performances to token-level accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "\n",
    "from conll import evaluate\n",
    "# for nice tables\n",
    "import pandas as pd\n",
    "\n",
    "# getting references (note that it is testb this time)\n",
    "refs = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testb')]\n",
    "print(refs[0])\n",
    "# getting hypotheses\n",
    "hyps = [hmm_ner.tag(s) for s in conll2002.sents('esp.testb')]\n",
    "print(hyps[0])\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Named Entity Recognition with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "txt = 'Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.'\n",
    "doc = nlp(txt)\n",
    "\n",
    "print([ent.text for ent in doc.ents])\n",
    "print([(t.ent_type_, t.ent_iob_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Shallow Parsing with Conditional Random Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CRFs](https://en.wikipedia.org/wiki/Conditional_random_field) are a type of __discriminative undirected probabilistic graphical model__. \n",
    "It is a generalization of __any__ undirected graph structure.\n",
    "In Natural Language Processing, the structure is a *sequence* of words, and conditioning is on *previous transition*. This is known as __Linear Chain CRFs__.\n",
    "\n",
    "For general graphs, the problem of exact inference in CRFs is intractable. For __Linear Chain CRFs__, however, there is an exact solution, and the used algorithm is \"analogous to the forward-backward and Viterbi algorithm for the case of Hidden Markov Models\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python CRF Tutorials\n",
    "\n",
    "Authors of the python packages provide tutorials in the form of notebooks already!\n",
    "\n",
    "__Follow the tutorials to learn the tools.__\n",
    "\n",
    "- [sklearn-crfsuite notebook](https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb)\n",
    "- [python-crfsuite notebook](https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb)\n",
    "\n",
    "- `bias` is explained [here](https://github.com/scrapinghub/python-crfsuite/issues/73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a CRF baseline for our dataset that:\n",
    "- considers only word itself and the previous tag (similar to HMM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set\n",
    "print(len(trn_sents))\n",
    "print(len(tst_sents))\n",
    "print(trn_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy & re-define feature extraction functions from the tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    return {'bias': 1.0, 'word.lower()': word.lower()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our baseline features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2features(trn_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trn_feats = [sent2features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_feats = [sent2features(s) for s in tst_sents]\n",
    "pred = crf.predict(tst_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use our `conll` evaluation script. (Notice that tools report token level metrics.)\n",
    "\n",
    "For that we will need to modify our prediction output a bit, to make it a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of CRFs lies in its ability to make use of rich feature representation. The process of extracting features from raw data is know as [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering).\n",
    "\n",
    "Common features used in sequence labeling with CRFs are:\n",
    "- part-of-speech tags\n",
    "- lemmas\n",
    "- token character prefixes and suffixes (e.g. first and last 1, 2, 3 characters of a word; `word[-3:]` in tutorial is suffix of length 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. SpaCy Token Features\n",
    "\n",
    "[spaCy](https://spacy.io/) provides a convenient way to augment our feature set with common features using in Natural Language Processing. \n",
    "\n",
    "The list of provided token-level features is available [here](https://spacy.io/api/token#attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Adding Features to CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify `sent2features` function to make use of spaCy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Spanish model of spacy\n",
    "!pip install es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import es_core_news_sm\n",
    "nlp = es_core_news_sm.load()\n",
    "\n",
    "# nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)  # to use white space tokenization (generally a bad idea for unknown data)\n",
    "\n",
    "def sent2spacy_features(sent):\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    feats = []\n",
    "    for token in spacy_sent:\n",
    "        token_feats = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower_,\n",
    "            'pos': token.pos_,\n",
    "            'lemma': token.lemma_\n",
    "        }\n",
    "        feats.append(token_feats)\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_feats = [sent2spacy_features(s) for s in trn_sents]\n",
    "trn_label = [sent2labels(s) for s in trn_sents]\n",
    "tst_feats = [sent2spacy_features(s) for s in tst_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trn_feats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(trn_feats, trn_label)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = crf.predict(tst_feats)\n",
    "\n",
    "hyp = [[(tst_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(tst_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exsecise is to experiment with a CRF model on NER task. You have to train and test a CRF with using different features on the conll2002 corpus (the same that we used here in the lab). \n",
    "\n",
    "The features that you have to experiment with are:\n",
    "- Baseline using the fetures in sent2spacy_features\n",
    "    - Train the model and print results on the test set\n",
    "- Add the \"suffix\" feature\n",
    "    - Train the model and print results on the test set\n",
    "- Add all the features used in the [tutorial on CoNLL dataset](https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb)\n",
    "    - Train the model and print results on the test set\n",
    "- Increase the feature window (number of previous and next token) to:\n",
    "    - `[-1, +1]`\n",
    "        - Train the model and print results on the test set\n",
    "    - `[-2, +2]`\n",
    "        - Train the model and print results on the test set\n",
    "\n",
    "        \n",
    "The format of the results has to be the table that we used so far, that is:\n",
    "```python\n",
    "results = evaluate(tst_sents, hyp)\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
