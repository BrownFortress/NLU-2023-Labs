{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca7d9e5",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "- pytorch \n",
    "    - Pytorch install: https://pytorch.org/get-started/locally/ \n",
    "- tqdm\n",
    "- sklearn\n",
    "- Huggingface Transformer: \n",
    "    - pip install transformers \n",
    "- **DATASET**:\n",
    "    - https://github.com/BrownFortress/IntentSlotDatasets\n",
    "    - We will use **ATIS** only\n",
    "    \n",
    "    \n",
    "\n",
    "# Outline\n",
    "\n",
    "#### Introduction\n",
    "- sequence labelling (Slot filling)\n",
    "- text classification (Intent classification)\n",
    "\n",
    "#### Preparing text for NN\n",
    "- word2id\n",
    "- special tokens \n",
    "- Customize Dataset class\n",
    "\n",
    "#### Split data in batches\n",
    "- Usage of Dataloader class\n",
    "- Padding sequences\n",
    "\n",
    "#### Neural Networks in Pytorch\n",
    "- Word emdbeddings\n",
    "- Implementation of an LSTM\n",
    "- Regularization techniques\n",
    "\n",
    "#### Train and Test a Neural Network\n",
    "- Optimizer\n",
    "- Loss function\n",
    "- Iteration over batches\n",
    "\n",
    "#### Hugging face library\n",
    "- Introduction and Usage\n",
    " \n",
    " \n",
    "# References\n",
    "- RNN: https://d2l.ai/chapter_recurrent-neural-networks/index.html \n",
    "- LSTM: https://d2l.ai/chapter_recurrent-modern/lstm.html\n",
    "- GRU: https://d2l.ai/chapter_recurrent-modern/gru.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a0554",
   "metadata": {},
   "source": [
    "# Sequence Labelling and Text classification tasks\n",
    "\n",
    "### Sequence Labelling\n",
    "The sequence labelling task is defined as:\n",
    "- Given a sequence of tokens $w = {w_1, w_2, ..., w_n}$,\n",
    "- defining a sequence of labels as $l = {l_1, l_2, ..., l_n}$\n",
    "- compute the sequence $\\hat{l}$ such as $\\hat{l} = \\underset{l}{\\operatorname{argmax}} P(l|w)$ \n",
    "\n",
    "In this lab session, we are going to see a particular case of sequence labelling task, which is named as Slot Filling (or Concept tagging). In this notebook, the **segmentation** (with IOB tags) and the **labelling** are done at the same time. \\\n",
    "\\\n",
    "An example is the following: \n",
    "\n",
    "| Slot Filling |  |                     |                     |  |  |  |  |\n",
    "|------------------|----|--------------------------|--------------------------|---|------|---|--------|\n",
    "| Input sequence:  | on | april                    | first                    | I | want | a | flight |\n",
    "| Output sequence: | O  | B-depart_date.month_name | B-depart_date.day_number | O | O    | O | O      |\n",
    "\n",
    "### Text classification\n",
    "The text classification problem is defined *(similar to Sequence Labelling)* as follows:\n",
    "- Given a sequence of tokens $w = {w_1, w_2, ..., w_n}$,\n",
    "- And a set of labels $L$ where $l \\in L$\n",
    "- estimate the label $\\hat{l}$ such as $\\hat{l} = \\underset{l}{\\operatorname{argmax}} P(l|w)$ \n",
    "\n",
    "The text classification task that we are going to see in this laboratory is named as Intent Classification. The Intent is an additional component of the *semantic frame*. \\\n",
    "\\\n",
    "An example is the following:\n",
    "\n",
    "| Intent Classification|  |                     |                     |  |  |  |  |\n",
    "|------------------|----|--------------------------|--------------------------|---|------|---|--------|\n",
    "| Input sequence:  | on | april                    | first                    | I | want | a | flight |\n",
    "| Output label: | flight     |\n",
    "\n",
    "\n",
    "# Dataset\n",
    "The dataset that we are going to use is ATIS (Airline Travel Information Systems). It is composed of trascriptions of humans asking about flight information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3289e",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "I have prepared a custom data structure for this dataset. The srtucture is the following:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "    \"utterance\": \"on april first i need a flight going from phoenix to san diego\", \n",
    "    \"slots\": \"O B-depart_date.month_name B-depart_date.day_number O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name\", \n",
    "    \"intent\": \"flight\"\n",
    "    },\n",
    "    \"...\"\n",
    " ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80808524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "import os\n",
    "device = 'cuda:0' # cuda:0 means we are using the GPU with id 0, if you have multiple GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # Used to report errors on CUDA side\n",
    "PAD_TOKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ea7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "        input: path/to/data\n",
    "        output: json \n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "tmp_train_raw = load_data(os.path.join('data','ATIS','train.json'))\n",
    "test_raw = load_data(os.path.join('data','ATIS','test.json'))\n",
    "print('Train samples:', len(tmp_train_raw))\n",
    "print('Test samples:', len(test_raw))\n",
    "\n",
    "pprint(tmp_train_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b3f37",
   "metadata": {},
   "source": [
    "### Create a dev set\n",
    "In the original split the developement set (dev set) is missing. To train and find the best hyperparameter of our network the dev set is fundamental. Thus, we have to create it starting from the **traning** set. The dev set is usually the 10% of the dataset. \\\n",
    "Possible sampling strategies:\n",
    "* Take the last n elements of the training set.\n",
    "* Do a random sampling from the training set.\n",
    "* Do a stratified sampling from the training set using one or more criteria. (The best way)\n",
    "    * For further details look [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Firt we get the 10% of dataset, then we compute the percentage of these examples \n",
    "# on the training set which is around 11% \n",
    "portion = round(((len(tmp_train_raw) + len(test_raw)) * 0.10)/(len(tmp_train_raw)),2)\n",
    "\n",
    "\n",
    "intents = [x['intent'] for x in tmp_train_raw] # We stratify on intents\n",
    "count_y = Counter(intents)\n",
    "\n",
    "Y = []\n",
    "X = []\n",
    "mini_Train = []\n",
    "\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1: # If some intents occure once only, we put them in training\n",
    "        X.append(tmp_train_raw[id_y])\n",
    "        Y.append(y)\n",
    "    else:\n",
    "        mini_Train.append(tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, Y, test_size=portion, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=Y)\n",
    "X_train.extend(mini_Train)\n",
    "train_raw = X_train\n",
    "dev_raw = X_dev\n",
    "\n",
    "y_test = [x['intent'] for x in test_raw]\n",
    "\n",
    "# Intent distribution\n",
    "print('Train:')\n",
    "pprint({k:round(v/len(y_train),3)*100 for k, v in sorted(Counter(y_train).items())})\n",
    "print('Dev:'), \n",
    "pprint({k:round(v/len(y_dev),3)*100 for k, v in sorted(Counter(y_dev).items())})\n",
    "print('Test:') \n",
    "pprint({k:round(v/len(y_test),3)*100 for k, v in sorted(Counter(y_test).items())})\n",
    "print('='*89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(train_raw))\n",
    "print('DEV size:', len(dev_raw))\n",
    "print('TEST size:', len(test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bbdbe",
   "metadata": {},
   "source": [
    "### Corvert words to numbers (word2id)\n",
    "Neural Netwoks in Pytorch, as in other libraries, work with numbers and vectors. (the value type can be both integers or floats)<br><br>\n",
    "**Exercise** *(5 minutes)*\n",
    "* Create a dictionary that maps the words in the training set to numbers.\n",
    "* Define the Dataset class to easily handle the train, dev and test sets.\n",
    "\n",
    "We will see later how to conver these indexes into vectors (aka embeddings).\n",
    "\n",
    "*Add special tokens \"pad\" and \"unk\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "w2id = {'pad':PAD_TOKEN}\n",
    "slot2id = {'pad':PAD_TOKEN}\n",
    "intent2id = {}\n",
    "# Map the words only from the train set\n",
    "# Map slot and intent labels of train, dev and test set. 'unk' is not needed.\n",
    "\n",
    "\n",
    "print('# Vocab:', len(w2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(slot2id)-1)\n",
    "print('# Intent:', len(intent2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be408b",
   "metadata": {},
   "source": [
    "### Lang class\n",
    "Later we will need to convert those numbers in the original form, so we need to invert those dictionaries. We create a calss named as Lang just for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Lang():\n",
    "    def __init__(self, words, intents, slots, cutoff=0):\n",
    "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "        \n",
    "    def w2id(self, elements, cutoff=None, unk=True):\n",
    "        vocab = {'pad': PAD_TOKEN}\n",
    "        if unk:\n",
    "            vocab['unk'] = len(vocab)\n",
    "        count = Counter(elements)\n",
    "        for k, v in count.items():\n",
    "            if v > cutoff:\n",
    "                vocab[k] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77bc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sum([x['utterance'].split() for x in train_raw], []) # No set() since we want to compute \n",
    "                                                            # the cutoff\n",
    "corpus = train_raw + dev_raw + test_raw # We do not wat unk labels, \n",
    "                                        # however this depends on the research purpose\n",
    "slots = set(sum([line['slots'].split() for line in corpus],[]))\n",
    "intents = set([line['intent'] for line in corpus])\n",
    "\n",
    "lang = Lang(words, intents, slots, cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e6097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35602d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5ac746e",
   "metadata": {},
   "source": [
    "### Customize the Dataset class\n",
    "In Pytorch the Dataset class helps you in handleing the dataset. The mandatory methods are ```__init__, __len__ and __getitem__```. <br>\n",
    "You can find more details here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class IntentsAndSlots (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, dataset, lang, unk='unk'):\n",
    "        self.utterances = []\n",
    "        self.intents = []\n",
    "        self.slots = []\n",
    "        self.unk = unk\n",
    "        \n",
    "        for x in dataset:\n",
    "            self.utterances.append(x['utterance'])\n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt = torch.Tensor(self.utt_ids[idx])\n",
    "        slots = torch.Tensor(self.slot_ids[idx])\n",
    "        intent = self.intent_ids[idx]\n",
    "        sample = {'utterance': utt, 'slots': slots, 'intent': intent}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_lab(self, data, mapper):\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "    \n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ab541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our datasets\n",
    "train_dataset = IntentsAndSlots(train_raw, lang)\n",
    "dev_dataset = IntentsAndSlots(dev_raw, lang)\n",
    "test_dataset = IntentsAndSlots(test_raw, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297a312",
   "metadata": {},
   "source": [
    "## Batches\n",
    "Batches are used to handle large datasets in the memory. Since the whole dataset cannot fit in GPU memories, we rashuffle the dataset and we split it in small batches that will be processed one at a time.\n",
    "### Padding\n",
    "Padding is a strategy to fit sequences of different lengths into a matrix. For instance:\n",
    "\n",
    "| Right padding|   |    |   |  |  |  |  \n",
    "|---|----|---|---|---|------|---|\n",
    "| I | saw| a | unk | with | a | telescope | \n",
    "| book | me | a | flight | [pad] | [pad] | [pad] | \n",
    "\n",
    "| Left padding|   |    |   |  |  |  |  \n",
    "|---|----|---|---|---|------|---|\n",
    "| I | saw| a | unk | with | a | telescope | \n",
    "| [pad] | [pad] | [pad] | book | me | a | flight | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874eb4c4",
   "metadata": {},
   "source": [
    "**Exercise** *(10 minuts)* <br> \n",
    "Write a function that adds padding on the right. (No need to convert the sentences to numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split them by white space\n",
    "sequences = ['I saw a man with a telescope', \n",
    "             'book me a flight', \n",
    "             'I want to see the flights from Milan to Ibiza']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b71d2",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "To split the dataset into batches and add padding we will use the DataLoader class. \n",
    "```python\n",
    "DataLoader(Dataset, batch_size=N, collate_fn={custom function}, shuffle=True)\n",
    "```\n",
    "*collate_fn* is used to shape the batch in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7105180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(data):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        # print(padded_seqs)\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "    data.sort(key=lambda x: len(x['utterance']), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
    "    src_utt, _ = merge(new_item['utterance'])\n",
    "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
    "    intent = torch.LongTensor(new_item[\"intent\"])\n",
    "    \n",
    "    src_utt = src_utt.to(device) # We load the Tensor on our seleceted device\n",
    "    y_slots = y_slots.to(device)\n",
    "    intent = intent.to(device)\n",
    "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
    "    \n",
    "    new_item[\"utterances\"] = src_utt\n",
    "    new_item[\"intents\"] = intent\n",
    "    new_item[\"y_slots\"] = y_slots\n",
    "    new_item[\"slots_len\"] = y_lengths\n",
    "    return new_item\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019e479",
   "metadata": {},
   "source": [
    "## Define a neural network in Pytorch\n",
    "In Pythorch the difinition of a neural network is quite flexible. In ```__init__``` the layer that is going to be used are instantiated. In ```forward```, the achitecture of the neural network is defined. Here you can find all the layers provided by Pytorch https://pytorch.org/docs/stable/nn.html while here you can find the recurrent layers https://pytorch.org/docs/stable/nn.html#recurrent-layers. \n",
    "\n",
    "<br><br>\n",
    "**pack_padded_sequence** and **pad_packed_sequences** are to compress and uncompress sequences in order to remove from the computation the padding embeddings. This reduces the computational cost which means speeding up training and therefore reducing CO2 emission.\n",
    " ![](https://i.stack.imgur.com/LPHAs.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class ModelIAS(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (ouput size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=False)    \n",
    "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        # Dropout layer How do we apply it?\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        utt_emb = utt_emb.permute(1,0,2) # we need seq len first -> seq_len X batch_size X emb_size\n",
    "        \n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        \n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy())\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input) \n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output)\n",
    "        # Get the last hidden state\n",
    "        last_hidden = last_hidden[-1,:,:]\n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: seq_len, batch size, calsses \n",
    "        slots = slots.permute(1,2,0) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992a22c",
   "metadata": {},
   "source": [
    "### Function to randomly initialize the weights\n",
    "This is a generic function that randomly initialize the parameters of RNN networks and linear layers. To dig deep in to this I would suggest you to look at here: https://pytorch.org/docs/master/nn.init.html \\\n",
    "\\\n",
    "*Note: In Pytorch every parameter of the network has a proper name like weight_ih, weight_hh etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fe3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a362fc",
   "metadata": {},
   "source": [
    "## Training set up\n",
    "Here we initialize the model and we select the hyperparamters of the neural network. Futhermore, we initialize the optimizer and we select the loss function.\\\n",
    "- You can find further optimization algorithms here: https://pytorch.org/docs/stable/optim.html\n",
    "- and further loss functions here: https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5edf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = ModelIAS(hid_size, out_slot, out_int, emb_size, vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a18f4",
   "metadata": {},
   "source": [
    "### Train Loop and Evaluation Loop\n",
    "We define two functions one for training our model and the other for evaluating it. To compute the performances on the slot filling task we will use the **conll script**, while for the intent classification task we are going to use the **classification_report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_loop(data, optimizer, criterion_slots, critenrion_intents, model):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        loss = loss_intent + loss_slot # In joint training we sum the losses. \n",
    "                                       # Is there another way to do that?\n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array\n",
    "\n",
    "def eval_loop(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot \n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:            \n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        \n",
    "    report_intent = classification_report(ref_intents, hyp_intents, \n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2eaee",
   "metadata": {},
   "source": [
    "## Train a neural network\n",
    "We train a neural network iterating several times over the training set. \n",
    "* **epochs**: number of times in which the whole training set is seen by the network\n",
    "* **early stopping**: keeps controlled the performance of the model on the dev set and interrupts the training when the performance is getting worse\n",
    "    * **patience**: wait for a number of step before interrupting the training, even though the performance is getting worse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "n_epochs = 200\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "for x in tqdm(range(1,n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                      criterion_intents, model)\n",
    "    if x % 5 == 0:\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                      criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        f1 = results_dev['total']['f']\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                         criterion_intents, model, lang)    \n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1466a",
   "metadata": {},
   "source": [
    "### Plot of the train and valid losses\n",
    "One of the techniques for debugging a neural network is to check the plot of the loss. If the loss goes smoothly down then the network works corretly, otherwise a deeper analysis is needed. Furthermore, this plot can be useful for deciding the learning rate and the optimizer algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
    "plt.title('Train and Dev Losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(sampled_epochs, losses_train, label='Train loss')\n",
    "plt.plot(sampled_epochs, losses_dev, label='Dev loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420df73c",
   "metadata": {},
   "source": [
    "### Multiple runs\n",
    "To have reliable results on small corpora we have to train and test the model from scratch for several times. At the end, we average the results and we compute the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "runs = 5\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    model = ModelIAS(hid_size, out_slot, out_int, emb_size, \n",
    "                     vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    criterion_intents = nn.CrossEntropyLoss()\n",
    "    \n",
    "    n_epochs = 200\n",
    "    patience = 3\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "    for x in range(1,n_epochs):\n",
    "        loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                          criterion_intents, model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                          criterion_intents, model, lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stoping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                             criterion_intents, model, lang)\n",
    "    intent_acc.append(intent_test['accuracy'])\n",
    "    slot_f1s.append(results_test['total']['f'])\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4565d",
   "metadata": {},
   "source": [
    " ![](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\n",
    "# Hugging Face\n",
    "Hugging Face is a library that allows you to used large pretrained models in an easy way. This means that you do not need to implement an architeture and train it from scratch. Hugging Face is also a community where people share trained models and code.\n",
    "<br/><br/>\n",
    "In Hugging Face there are many different models (https://huggingface.co/models) that you can import and each of them has its own input and output shapes. However, Transformer-based models are usually composed of two parts: \n",
    "- **Tokenizer**\n",
    "- **Architecture/Pretrained model**\n",
    "\n",
    "The **tokenizers** used by Transformer-based models are different from those we saw in the lab. While for instance Spacy's tokenizer is rule-based and splits the text looking at the punctuation, the goal of Transformer tokenizers is to reduce the vocabulary length by spliting words into subwords. To do this, several algorithms have been proposed. If you are interested in this topic you can find a thoroughly explanation here: https://huggingface.co/docs/transformers/tokenizer_summary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT model script from: huggingface.co\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"I saw a man with a telescope\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fad3c",
   "metadata": {},
   "source": [
    "# Exercise (2 points)\n",
    "Modify the baseline architecture Model IAS in an addition way:\n",
    "- Add bidirectionality\n",
    "- Add dropout layer\n",
    "\n",
    "***Dataset to use: ATIS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce570a7f-ce03-4940-9c46-195fe6dd5b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a30eb2ce-9952-4127-b258-3d372f452c2b",
   "metadata": {},
   "source": [
    "# Exercise (4 points)\n",
    "Fine-tune BERT model using a multi-task learning setting on intent classification and slot filling. \n",
    "<br>\n",
    "You can refer to this paper to have a better understanding of such model: https://arxiv.org/abs/1902.10909\n",
    "\n",
    "***Dataset to use: ATIS***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef9c39-49e5-4517-83d8-0255a968e0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
