{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Understanding: \n",
    "    - different methods of representing words as vectors\n",
    "    - vectors and similarity between vectors\n",
    "    - evaluation of word embeddings\n",
    "    \n",
    "- Learning how to:\n",
    "    - train word embeddings with gensim\n",
    "    - use pre-trained word embeddings for similarity computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 6: Vector Semantics and Embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [pytorch](https://pytorch.org/get-started/locally/)\n",
    "- tqdm\n",
    "- matplotlib\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Recommended Reading*:\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "\n",
    "*Notebook Covers Material of*:\n",
    "- [SLP](https://web.stanford.edu/~jurafsky/slp3/6.pdf) Chapter 6: Vector Semantics and Embeddings\n",
    "\n",
    "__Requirements__\n",
    "\n",
    "- spaCy\n",
    "- [gensim](https://radimrehurek.com/gensim/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Words as Vectors (Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), [**word embedding**](https://en.wikipedia.org/wiki/Word_embedding) is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word embeddings is the process by which words are transformed into vectors of (real) numbers.\n",
    "- Definition of meaning by distributional similarity / usage: similar words are close in \"space\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. One-Hot Encoding\n",
    "- sparse vectors\n",
    "- most basic way to turn a token into a vector\n",
    "- method\n",
    "    - associate a unique integer index with every word in a vocabulary of size $V$\n",
    "    - turn this integer index $i$ into a binary vector of size $V$ (i.e. the size of the vocabulary)\n",
    "    - the vector has all values `0` except for the $i$th entry, which is `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Co-Occurence Matrices and Word as Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Term-Document Matrix\n",
    "- could be used to represent words, where dimension are documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. TF-IDF\n",
    "- sparse vectors\n",
    "- generally used to represent documents, where dimensions are words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### TF: Term Frequency\n",
    "$$\\text{tf}_{t,d} = \\text{count}(t,d)$$\n",
    "$$\\text{tf}_{t,d} = \\log_{10}(\\text{count}(t,d) + 1)$$\n",
    "\n",
    "`+1` is because log of 0 is undefined.\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "$$\\text{tf}_{t,d} = \n",
    "\\begin{cases}\n",
    "1 + \\log_{10}(\\text{count}(t,d)), & \\text{if count}(t,d) > 0\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### IDF: Inverse Document Frequency\n",
    "\n",
    "$$\\text{idf}_t = \\frac{N}{\\text{df}_t}$$\n",
    "\n",
    "Usually in log space, like term frequency.\n",
    "\n",
    "$$\\text{idf}_t = \\log_{10}(\\frac{N}{\\text{df}_t})$$\n",
    "\n",
    "- $\\text{df}_t$ is the number of documents in which term $t$ occurs\n",
    "- $N$ is the total number of documents in the collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The __tf-idf__ weighted value $w_{t,d}$ for word $t$ in document $d$ is the combination of $\\text{tf}_{t,d}$ and $\\text{idf}_t$:\n",
    "\n",
    "$$w_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Term-Term Matrix\n",
    "- a.k.a. \"word-word\" or \"word-context\" matrix\n",
    "- words are represented by a function of the counts of nearby words \n",
    "- size $|V| \\times |V|$, where $V$ is the vocabulary size\n",
    "    - usually context is taken to be a document or words in a window around the target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.4. Pointwise Mutual Information (PMI) and Positive Pointwise Mutual Information (PPMI)\n",
    "- used for term-term matrices\n",
    "- \"the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.1. Pointwise Mutual Information (PMI)\n",
    "- a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent:\n",
    "\n",
    "$$I(x, y) = \\log_2 \\frac{P(x, y)}{P(x)P(y)}$$\n",
    "\n",
    "\n",
    "The pointwise mutual information between a target word $w$ and a context word $c$ is defined as:\n",
    "\n",
    "$$\\text{PMI}(w, c) = \\log_2 \\frac{P(w, c)}{P(w)P(c)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.2. Positive Pointwise Mutual Information (PMI)\n",
    "- PMI values range from negative to positive infinity.\n",
    "- negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable\n",
    "- it is more common to use Positive PMI (called PPMI) which replaces all negative PMI values with zero\n",
    "\n",
    "$$\\text{PPMI}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P(c)}, 0)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.4.3. PPMI Matrix\n",
    "To get a PPMI matrix from a co-occurrence matrix $F$, where $W$ rows are words and $C$ columns are contexts, and $f_{ij}$ is the number of times word $w_i$ appears in context $c_j$ (i.e. value of the cell).\n",
    "\n",
    "$$P(w,c) = \\frac{f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(w) = \\frac{\\sum_{j=1}^C f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(c) = \\frac{\\sum_{i=1}^W f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- PMI has the problem of being biased toward infrequent events: very rare words tend to have very high PMI values.\n",
    "- Thus, $P(c)$ is computed as $P_{\\alpha}(c)$ that raises the probability of the context word to the power of $\\alpha$ (e.g. $0.75$)\n",
    "    - Alternative is Laplace smoothing\n",
    "\n",
    "$$\\text{PPMI}_{\\alpha}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P_{\\alpha}(c)}, 0)$$\n",
    "\n",
    "$$P_{\\alpha}(c) = \\frac{\\text{count}(c)^{\\alpha}}{\\sum_{c}\\text{count}(c)^{\\alpha}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3. Training Word Embeddings with `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3.1. Word2Vec\n",
    "- dense vectors\n",
    "- representation is created by training a classifier to distinguish nearby and far-away words\n",
    "- Variants\n",
    "    - SKIP-GRAM\n",
    "    - CBOW\n",
    "- Refer to [documentation](https://radimrehurek.com/gensim/models/word2vec.html) for details\n",
    "- [Tutorial](https://rare-technologies.com/word2vec-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip install python-Levenshtein\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "from gensim.models import Word2Vec\n",
    "data = ['Iceland is faraway from Padova', 'Rome is the capital of Italy', 'Paris is a big city']\n",
    "model = Word2Vec(sentences=[d.split() for d in data], vector_size=10, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# getting word vectors\n",
    "print(model.wv['Rome'])\n",
    "# getting most similar\n",
    "print(model.wv.most_similar('Rome', topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Vector Similarity\n",
    "- two words are similar in meaning if their context __vectors__ are similar\n",
    "- __Cosine similarity__ measures the similarity between two vectors of an __inner product space__. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Dot Product\n",
    "\n",
    "- dot product (inner product)\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = \\sum^N_{i=1}v_i w_i = v_1 w_1 + v_2 w_2 + ... + v_N w_N$$\n",
    "\n",
    "- vector length (L2 norm $||v||_2$)\n",
    "\n",
    "$$|\\vec{v}| = \\sqrt{\\sum^N_{i=1} v_i^2}$$ \n",
    "\n",
    "$$ |\\vec{v}| = \\sqrt{\\vec{v}\\cdot\\vec{v}} = \\sqrt{\\sum^N_{i=1} v_i v_i} = \\sqrt{\\sum^N_{i=1} v_1 v_1 + v_2 v_2 + ... + v_N v_N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Cosine Similarity\n",
    "\n",
    "- L2 normalized dot product of 2 vectors\n",
    "    - $\\theta$ is the angle between $\\vec{v}$ and $\\vec{w}$\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = |\\vec{v}||\\vec{w}|\\cos\\theta$$\n",
    "\n",
    "$$\\cos\\theta = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|}$$\n",
    "\n",
    "$$\\text{CosSim}(\\vec{v},\\vec{w}) = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|} = \\frac{\\sum^N_{i=1}v_i w_i}{\\sqrt{\\sum^N_{i=1} v_i^2} \\sqrt{\\sum^N_{i=1} w_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Cosine Distance\n",
    "$$\\text{Cosine Distance}(\\vec{v}, \\vec{w}) = 1 - \\text{Cosine Similarity}(\\vec{v}, \\vec{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises\n",
    "- Implement a function to compute __cosine similarity__ using `numpy` methods\n",
    "    - `np.dot`\n",
    "    - `norm`\n",
    "- Using the defined functions\n",
    "    - compute cosine similarity between two word embeddings for instance `Rome` and `city` or `Paris` and `Rome`\n",
    "    - compare similarity values to the cosine similarity using the ouput of (`scipy.spatial.distance.cosine`)\n",
    "        - i.e. use *distance* to compute *similarity*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(v, w):\n",
    "    return np.dot(v,w)/(norm(v)*norm(w))\n",
    "\n",
    "rome = model.wv['Rome']\n",
    "paris = model.wv['Paris']\n",
    "print(cosine_similarity(rome, paris))\n",
    "print(1 - cosine(rome, paris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Pre-Trained Embeddings\n",
    "- Training embeddings is computationally expensive\n",
    "- Many pre-trained models are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "# Download the 'word2vec-google-news-300' embeddings\n",
    "# w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.1. Word Embeddings in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> To make them compact and fast, spaCy's small pipeline packages (all packages that end in `sm`) don't ship with word vectors, and only include context-sensitive tensors. This means you can still use the `similarity()` methods to compare documents, spans and tokens -- but the result won't be as good, and individual tokens won't have any vectors assigned. So in order to use real word vectors, you need to download a larger pipeline package:\n",
    "\n",
    "> `python -m spacy download en_core_web_lg`\n",
    "\n",
    "> Pipeline packages that come with built-in word vectors make them available as the `Token.vector` attribute. `Doc.vector` and `Span.vector` will default to an __average of their token vectors__. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.\n",
    "\n",
    "> Each `Doc`, `Span`, `Token` and `Lexeme` comes with a `.similarity` method that lets you compare it with another object, and determine the similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.1.1. Accessing Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "txt = 'Rome is the capital of Italy'\n",
    "doc = nlp(txt)\n",
    "\n",
    "tok = doc[0]  # let's get Rome\n",
    "\n",
    "print(\"string:\", tok.text)\n",
    "# print(\"vector:\", tok.vector)\n",
    "print(\"vector dimension:\", len(tok.vector))\n",
    "print(\"spacy vector norm:\", tok.vector_norm)\n",
    "print(\"numpy vector norm:\", np.sqrt(np.dot(tok.vector, tok.vector)))\n",
    "print(\"numpy linalg norm:\", np.linalg.norm(tok.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# let's get Paris & compare its vector to rome\n",
    "paris = nlp('Paris')[0]\n",
    "print(paris.text)\n",
    "\n",
    "print(\"spacy CosSim({}, {}):\".format(tok.text, paris.text), tok.similarity(paris))\n",
    "print(\"scipy CosSim({}, {}):\".format(tok.text, paris.text), 1 - cosine(tok.vector, paris.vector))\n",
    "print(\"_our_ CosSim({}, {}):\".format(tok.text, paris.text), cosine_similarity(tok.vector, paris.vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Train your own Word Embeddings\n",
    "One way to train word embeddings is to use a language model. We have already seen language models in Lab 3, but now we are going to develop a language model using a neural architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Task definition\n",
    "To model the probaiblity distribution over a sequence, we are going to use the Chain Rule as we have seen in LAB 3:\n",
    "$$P(w_{1}^{n}) = P(w_1) P(w_2|w_1) P(w_3|w_1^2) ... P(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{P(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "However, at that time we have used ngram to trucate the previous context ($N-1$), in order to compute meaningfull probabilities. While using neural models, we will let the model to decide by itself how to manage the previous contex and thus which are the tokens relevant for the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 RNNs are the most suitable architacture\n",
    "One of most suitable neural architecture for the Language Model task is the Recurrent Neural Network. The architecture is composed of a RNN layer (vanilla, LSTM, GRU) and a softmax that outputs the probability over the dictionary. Indeed the size of the output vector is equal to the size of the dictionary, i.e. the model cannot predict tokens that are not present in vocabularly. <br>\n",
    "> LM task in RNN can be tackled as a sequence labelling task (i.e. len of input and output sequence are always the same) in which the input sequence is $ input = \\{w_1, w_2, w_{n-1}\\}$ and the output is $ output = \\{w_2, w_2, w_{n}\\}$\n",
    ">\n",
    "> **Example** our sentence is ***\"I go to Miami\"*** the input sequence would be ***\"I go to\"*** and the output is ***\"go to Miami\"***. \n",
    ">\n",
    "> Notice: \n",
    "> - To proper model the sequence probabilities we need add boundary markers \\<s\\> and \\</s\\>.\n",
    "> - However in LM RNN only the end of sentence token \\</s\\>  is usually used unless we need for some reason (e.g. in ASR) to compute the probability distribution of the first token of a sentence. \n",
    "\n",
    "<img src=\"https://i.postimg.cc/zGH99MFY/rnn-lm.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "In the image below you can see a working example of a language model with RNN. \n",
    "\n",
    "<img src=\"https://i.postimg.cc/fydQNrYP/LM-RNN.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# RNN Elman version\n",
    "# We are not going to use this since for efficienty purposes it's better to use the RNN layer provided by pytorch  \n",
    "\n",
    "class RNN_cell(nn.Module):\n",
    "    def __init__(self,  hidden_size, input_size, output_size, vocab_size, dropout=0.1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.W = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, prev_hidden, word):\n",
    "        input_emb = self.W(word)\n",
    "        prev_hidden_rep = self.U(prev_hidden)\n",
    "        # ht = σ(Wx + Uht-1 + b)\n",
    "        hidden_state = self.sigmoid(x + prev_hidden_rep)\n",
    "        # yt = σ(Vht + b)\n",
    "        output = self.output(hidden_state)\n",
    "        return hidden_state, output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LM_RNN, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab \n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False)    \n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        rnn_out, _  = self.rnn(emb)\n",
    "        output = self.output(rnn_out).permute(0,2,1)\n",
    "        return output\n",
    "    def get_word_embedding(self, token):\n",
    "        return self.embedding(token).squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    def get_most_similar(self, vector, top_k=10):\n",
    "        embs = self.embedding.weight.detach().cpu().numpy()\n",
    "        #Our function that we used before\n",
    "        scores = []\n",
    "        for i, x in enumerate(embs):\n",
    "            if i != self.pad_token:\n",
    "                scores.append(cosine_similarity(x, vector))\n",
    "        # Take ids of the most similar tokens \n",
    "        scores = np.asarray(scores)\n",
    "        indexes = np.argsort(scores)[::-1][:top_k]  \n",
    "        top_scores = scores[indexes]\n",
    "        return (indexes, top_scores)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Data loading \n",
    "For sake of time we are going to see this part in detail in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, eos_token=\"<eos>\"):\n",
    "    output = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            output.append(line + eos_token)\n",
    "    return output\n",
    "\n",
    "def get_vocab(corpus, special_tokens=[]):\n",
    "    output = {}\n",
    "    i = 0 \n",
    "    for st in special_tokens:\n",
    "        output[st] = i\n",
    "        i += 1\n",
    "    for sentence in corpus:\n",
    "        for w in sentence.split():\n",
    "            if w not in output:\n",
    "                output[w] = i\n",
    "                i += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = read_file(\"dataset/ptb.train.txt\")\n",
    "dev_raw = read_file(\"dataset/ptb.valid.txt\")\n",
    "test_raw = read_file(\"dataset/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab is computed only on training set \n",
    "# However you can compute it for dev and test just for statistics about OOV \n",
    "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        \n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "        \n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_seq(self, data, lang): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "    \n",
    "    new_item[\"source\"] = source.to(device)\n",
    "    new_item[\"target\"] = target.to(device)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=512, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Train and validate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train_loop(data, optimizer, criterion, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    \n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "        \n",
    "    return sum(loss_array)/sum(number_of_tokens)\n",
    "\n",
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            loss = eval_criterion(output, sample['target'])\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "            \n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Experiment also with a smaller or bigger model by changing hid and emb sizes \n",
    "# A large model tends to overfit\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "# Don't forget to experiment with a lower training batch size\n",
    "\n",
    "# With SGD try with an higer learning rate\n",
    "lr = 0.0001 # This is definitely not good for SGD\n",
    "clip = 5 # Clip the gradient\n",
    "device = 'cuda:0'\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_ppl = math.inf\n",
    "best_model = None\n",
    "pbar = tqdm(range(1,n_epochs))\n",
    "for epoch in pbar:\n",
    "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)    \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        sampled_epochs.append(epoch)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "        if  ppl_dev < best_ppl: # the lower, the better\n",
    "            best_ppl = ppl_dev\n",
    "            best_model = copy.deepcopy(model).to('cpu')\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "            \n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "                          \n",
    "best_model.to(device)\n",
    "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)    \n",
    "print('Test ppl: ', final_ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7 Evaluation: Analogy Task\n",
    "In the word analogy task, we complete the sentence of the form\n",
    "\n",
    "\"$w_1$ is to $w_2$ as $w_3$ is to $w4$\", where $w_4$ is a blank. \n",
    "\n",
    "For instance:\n",
    "\n",
    "\"*man* is to *woman* as *king* is to **__**\", and our goal is to guess the missing word (*queen*)\n",
    "\n",
    "The task is approached using cosine similarity between vector differences: \n",
    "\n",
    "$$\\vec{w_2} - \\vec{w_1} \\approx \\vec{w_4} - \\vec{w_3}$$\n",
    "\n",
    "$$\\vec{w_4} \\approx = \\vec{w_3} + \\vec{w_2} - \\vec{w_1}$$\n",
    "\n",
    "$$w = \\arg\\max_{w \\in V}(\\vec{w} \\cdot (\\vec{w_3} + \\vec{w_2} - \\vec{w_1}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$w = \\arg\\max_{w \\in V}\\text{CosSim}(\\vec{w_2} - \\vec{w_1}, \\vec{w} - \\vec{w_3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Analogy using Most Similar\n",
    "> For each of the given vectors, find the `n` most similar entries to it by cosine. \n",
    "Queries are by vector. Results are returned as a (`keys`, `best_rows`, `scores`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def analogy_spacy(w1, w2, w3):\n",
    "    v1 = nlp.vocab[w1].vector\n",
    "    v2 = nlp.vocab[w2].vector\n",
    "    v3 = nlp.vocab[w3].vector\n",
    "    \n",
    "    # relation vector\n",
    "    rv = v3 + v2 - v1\n",
    "   \n",
    "    # n=1 & sorted by default\n",
    "    ms = nlp.vocab.vectors.most_similar(np.asarray([rv]), n=10)\n",
    "    \n",
    "    # getting words & scores\n",
    "    for i, key in enumerate(ms[0][0]):\n",
    "        print(nlp.vocab.strings[key], ms[2][0][i])\n",
    "        \n",
    "def analogy_our_model(w1, w2, w3, model, lang):\n",
    "    model.eval().to('cpu')\n",
    "    tmp_w1 = torch.LongTensor([lang.word2id[w1] if w1 in lang.word2id else lang.word2id['<unk>']]) \n",
    "    tmp_w2 = torch.LongTensor([lang.word2id[w2] if w2 in lang.word2id else lang.word2id['<unk>']])\n",
    "    tmp_w3 = torch.LongTensor([lang.word2id[w3] if w3 in lang.word2id else lang.word2id['<unk>']])\n",
    "    \n",
    "    v1 = model.get_word_embedding(tmp_w1)\n",
    "    v2 = model.get_word_embedding(tmp_w2)\n",
    "    v3 = model.get_word_embedding(tmp_w3)\n",
    "    # relation vector\n",
    "    rv = v3 + v2 - v1\n",
    "\n",
    "    # n=1 & sorted by default\n",
    "    ms = model.get_most_similar(rv)\n",
    "\n",
    "    \n",
    "    # getting words & scores\n",
    "    for i, key in enumerate(ms[0]):\n",
    "        print(lang.id2word[key], ms[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analogy_our_model('man', 'woman', 'u.s.', model, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(analogy_spacy('man', 'woman', 'king'))\n",
    "# expected output ('Queen', 0.7881)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print(analogy_spacy('Rome', 'Italy', 'Paris'))\n",
    "# ('france', 0.7606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1 (2 points)\n",
    "Modify the baseline LM_RNN (the idea is to add a set of improvements and see how these affect the performance). Furthremore, you have to play with the hyperparameters to minimise the PPL and thus print the results achieved with the best configuration. Here are the links to the state-of-the-art papers which uses vanilla RNN [paper1](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947611), [paper2](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf). \n",
    "\n",
    "- Replace RNN with LSTM (output the PPL)\n",
    "- Add two dropout layers: (output the PPL)\n",
    "    - one on embeddings, \n",
    "    - one on the output\n",
    "- Replace SGD with AdamW (output the PPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (4 points)\n",
    "Add the following regularizations described in [this paper](https://openreview.net/pdf?id=SyyGPP0TZ):\n",
    "- Weight Tying\n",
    "- Variational Dropout\n",
    "- Non-monotonically Triggered AvSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
